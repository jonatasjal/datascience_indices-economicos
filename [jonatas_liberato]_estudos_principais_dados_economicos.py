# -*- coding: utf-8 -*-
"""[Jonatas-Liberato] estudos-principais-dados-economicos

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11-Ga9TpcKDm0Pne4qdgj4TWA0Kt2nSvI

# **Gerando .csv**

**Localmente**
"""

# Lendo um dataset de fundos
# dataset = inv.search_funds(by='name', value='alaska black')
# dataset

# Tipos de colunas
# dataset.columns.values

# Novo dataset
# novodataset = dataset.filter(items = ['country', 'issuer', 'asset_class'])
# novodataset

# Biblioteca necessária
# from google.colab import files

# novodataset.to_csv('arquivo.csv') 
# files.download('arquivo.csv')

"""**Para o Google Planilhas**"""

!pip install gspread

!pip install --upgrade --quiet gspread

# Bibliotecas
from google.colab import auth
import gspread
from oauth2client.client import GoogleCredentials

# Autenticação
auth.authenticate_user()
autenticacao = gspread.authorize(GoogleCredentials.get_application_default())

# Método 2
from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

# sh = gc.create('planilhateste')

# Open our new sheet and add some data.
# worksheet = gc.open('A new spreadsheet').sheet1

# cell_list = worksheet.range('A1:C2')

# import random
# for cell in cell_list:
#   cell.value = random.randint(1, 10)

# worksheet.update_cells(cell_list)
# Go to https://sheets.google.com to see your new spreadsheet.

"""Referência: https://www.youtube.com/watch?v=YaCeqXpxWUk"""

# Criando planilha vazia
# gc.create('planilhateste')

# Abrindo a planilha
# planilha = gc.open('planilhateste')

# Adicionando página (nome da página, nº de linhas e nº colunas)
# planilha.add_worksheet('novapagina', 10, 5)

# Selecionando uma página (página por índice nesse caso 0)
# pagina1 = planilha.get_worksheet(0)

# Apagando
# planilha.del_worksheet(pagina1)

# Alterando os valores das células
# 1º seleciona a página
# nova_pagina = planilha.get_worksheet(0) # primeira linha

# Adicionando o valor - 1º método
# nova_pagina.update_acell('a1', 120) # célula e valor

# 2º método
# nova_pagina.update_cell(3, 3 , 'valor')

# Alterando várias células 
# Primeiro se cria um intervalo
# lista_cel = nova_pagina.range('a1:b2')
# lista_cel

# Valores a serem inseridos
# valores = ['a', 'b', 'c', 'd']
# for i, valor in enumerate(valores):
#   lista_cel[i].value = valor
# lista_cel

# Passando o range
# nova_pagina.update_cells(lista_cel)

"""# **1 - USANDO BIBLIOTECAS DA INVESTING.COM**

----

# Teste com Fundos
"""

# Instalando biblioteca da Investing.com
!pip install investpy

import investpy as inv
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns; sns.set()
import matplotlib
matplotlib.rcParams['figure.figsize'] = (16,8)

# Lista com fundos do Brasil
fundosBR = inv.get_funds_list(country='brazil')
fundosBR

len(fundosBR)

fundosBR[:5]

# Pesquisando um fundo específico
pesquisa = inv.search_funds(by='name', value='alaska black')
pesquisa

pesquisa['name'][0]

# Pegando fundo específico e baixando os dados do fundo
fundo = 'Alaska Black Fundo De Investimento Em Cotas De Fundos De Investimento Em Ações - Bdr Nível I'
alaska = inv.get_fund_historical_data(fundo, country='brazil', from_date='01/01/2000', to_date='01/05/2021')['Close']
alaska

# Plotando o fundo
alaska.plot()

# Pesquisnando outro fundo
pesquisa2 = inv.search_funds(by='name', value='ip parti')
pesquisa2

pesquisa2['name'][1]

fundo = 'Ip Participações Ipg Fundo De Investimento Em Cotas De Fundos De Investimento Em Ações Bdr Nível I'
ip = inv.get_fund_historical_data(fundo, country='brazil', from_date='01/10/2020', to_date='01/01/2022')['Close']
ip

ip.plot()

"""**Teste com ETFs**"""

inv.get_etfs_overview(country='brazil')

bova11 = inv.get_etf_historical_data('Ishares Ibovespa', country='brazil', from_date='01/10/2008', to_date='01/01/2022')['Close']
bova11

bova11.plot()

# Comparando os 3 fundos
comparativo = pd.DataFrame()
data_inicio = ip.index[0] # usamos a data deste fundo como referência por ser mais recente

comparativo['ip'] = ip / ip.loc[data_inicio]
comparativo['alaska'] = alaska / alaska.loc[data_inicio]
comparativo['bova11'] = bova11 / bova11.loc[data_inicio]

comparativo

# Plot
comparativo.plot()

"""# Teste com Índices"""

# Lista com índices do Brasil
indices = inv.get_indices_list(country='brazil')
indices

bovespa_recentes = inv.get_index_recent_data('Bovespa', country = 'brazil')
bovespa_recentes

bovespa_historicos = inv.get_index_historical_data('Bovespa', country = 'brazil', from_date='01/01/1990', to_date='24/05/2022')
bovespa_historicos

bovespa_dolarizado = inv.get_index_historical_data('Ibovespa USD', country = 'brazil', from_date='01/01/2018', to_date='24/05/2022')
bovespa_dolarizado

# Comparando os índices
comparativo = pd.DataFrame()
data_inicio = bovespa_recentes.index[0] # indice com data mais recente como referência

comparativo['bovespa_recentes'] = bovespa_recentes['Close'] / bovespa_recentes['Close'].loc[data_inicio]
comparativo['bovespa_historicos'] = bovespa_historicos['Close'] / bovespa_historicos['Close'].loc[data_inicio]
#comparativo['bovespa_dolarizado'] = bovespa_dolarizado['Close'] / bovespa_dolarizado['Close'].loc[data_inicio]

comparativo

comparativo.plot()

"""# Teste com Ações"""

acoes = inv.get_stocks('brazil')
acoes

# Descrição da empresa e o link para a empresa no site da Investing
bradesco = inv.get_stock_company_profile('bbdc4', country = 'brazil')
bradesco

bradesco = inv.get_stock_historical_data('bbdc4', country = 'brazil', from_date = '01/01/2008', to_date = '01/05/2022')['Close']
bradesco

# Informações da empresa
bradesco = inv.get_stock_information('bbdc4', country = 'brazil')
bradesco

# Indicadores técnicos das ações
bradesco = inv.technical_indicators('bbdc4', country = 'brazil', product_type='stock')
bradesco

# Médias móveis
bradesco = inv.moving_averages('bbdc4', country = 'brazil', product_type='stock')
bradesco

"""# Teste com Curva de juros"""

# Prazos dos juros do Brasil
bondsBR = inv.get_bonds_list('brazil')
bondsBR

bonds_overview = inv.get_bonds_overview('brazil')
bonds_overview

plt.plot(bondsBR, bonds_overview['last_close'])

# Estabelecendo prazos para futuras consultas
data_incio = '01/01/2010'
data_fim = '01/05/2022'

# 1 ANO
um_ano = inv.get_bond_historical_data('Brazil 1Y', from_date = data_incio, to_date = data_fim)
um_ano

# 8 ANOS
oito_anos = inv.get_bond_historical_data('Brazil 8Y', from_date = data_incio, to_date = data_fim)
oito_anos

# Plot
um_ano['Close'].plot()

# Verificando todos os títulos
data_inicio = '01/01/2010'
data_fim = '28/04/2020'
bonds = pd.DataFrame()

for prazo in bondsBR:
  bonds[prazo] = inv.get_bond_historical_data(prazo, from_date = data_inicio, to_date = data_fim)['Close']

bonds

plt.plot(bondsBR, bonds.loc['2020-04-23'].values)

"""**Importando uma biblioteca gráica que permite interação**"""

import plotly.graph_objects as go

#fig = go.Figure()

#for i in bonds.index:
#  fig.add_trace(go.Scatter(x = bondsBR, y = bonds.loc[i], mode = 'lines', name = str(i)))
#  fig.show()

# Diminuindo o número de curvas e pegando taxas a cada mês(fechamento)
data_inicio = '01/01/2010'
data_fim = '28/04/2020'
bondsMensal = pd.DataFrame()

for prazo in bondsBR:
  bondsMensal[prazo] = inv.get_bond_historical_data(prazo, from_date = data_inicio, to_date = data_fim, interval = 'Monthly')['Close']

bondsMensal

fig = go.Figure()

for i in bondsMensal.index:
  fig.add_trace(
      go.Scatter(
          x = bondsBR, 
          y = bondsMensal.loc[i], 
          mode = 'lines', 
          name = str(i),
          visible = False
          )
      )
  
# exibindo a primeira cruva
fig.data[0].visible = True

steps = []

# Percorrendo todas as curvas da figura
for i in range(len(fig.data)):
  step = dict( # dicionario
      method = 'restyle',
      args = ['visible', [False] * len(fig.data)],
      label=fig.data[i]['name'][:7] # um slicing para pegar somente ano e mês dos caracteres das datas
  )
  step['args'][1][i] = True # preenchendo o args do step e com [1][i] para deixá-lo visível
  steps.append(step) # o que criamos, acrescentamos a linha steps

# criamos um slider para rolar de um lado para o outro
sliders = [dict(
    active = 0,
    currentvalue = {'prefix': 'Mês:'},
    pad = {'t':50},
    steps = steps
  )]

fig.update_layout(
    sliders=sliders,
    yaxis = dict(range=[3, 14.5]) # definindo a escala do eixo y, o máximo e o mínimo
)

fig.show()

# Consulta do Banco Central
def consulta_bc(codigo_bcb):
  url = 'http://api.bcb.gov.br/dados/serie/bcdata.sgs.{}/dados?formato=json'.format(codigo_bcb)
  df = pd.read_json(url)
  df['data'] = pd.to_datetime(df['data'], dayfirst=True)
  df.set_index('data', inplace = True)
  return df

# Baixando a meta da SELIC
selic_meta = consulta_bc(432)

s2015 = selic_meta[selic_meta.index.year >= 2015] # a partir de 2015
b2015 = bonds[bonds.index.year >= 2015] # bonds desde 2015

fig = go.Figure()

for i in bondsBR:
  # Bonds
  fig.add_trace(
      go.Scatter(
          x = b2015.index, 
          y = b2015[i],
          mode = 'lines', 
          name = str(i)))
  
  # SELIC
  fig.add_trace(
      go.Scatter(
          x = s2015.index, 
          y = s2015['valor'], 
          mode = 'lines', 
          name = 'SELIC', 
          line={'color': 'black'}))
  fig.show()

"""# Teste de Fundos de Investimentos pela CVM

**Base de dados**: http://dados.cvm.gov.br

*Buscando dados no site da CVM e retornando um DataFrame Pandas site.*

*Buscamos os informes diários:*
http://dados.cvm.gov.br/dataset/fi-doc-inf_diario
"""

# Importando o Pandas e definindo uma largura maior para a coluna
#import pandas as pd
pd.set_option('display.max_colwidth', 150)
#ped.options.display.float_format = '{:.2f}'.format

"""**Como o arquivo está zipado, faremos o tratamento para utilizá-lo**"""

# Extraindo arquivos zipados
import zipfile # manipula arquivos zip
import requests #biblioteca HTTP simples
from io import BytesIO #processamento E/S de vários tipos e fluxos de dados
import os # maneira simples de criar pastar e gerenciá-las

# Criando diretório para o arquivo
os.makedirs('./cvm_maio2022', exist_ok = True)

# Analisando a estrutura do arquivo e os separadores
#arquivo = pd.read_csv('./cvm_maio2022/inf_diario_fi_202205.csv')
#arquivo

# Se o arquivo estiver em .txt
# arquivo - pd.read_csv(...caminho, sep = '', decimal = '')

# Descobrindo os tipos das colunas
#dict(arquivo.dtypes)

# Descompactando  e salvando o zip
url = 'http://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_202205.zip'
filebytes = BytesIO(
    requests.get(url).content
)

myzip = zipfile.ZipFile(filebytes)
myzip.extractall('./cvm_maio2022')

"""**Buscando os dados no site da CVM**"""

def busca_informes_cvm(ano, mes):
  url = 'http://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_{:02d}{:02d}.csv'.format(ano,mes) # colocando o 0 na frente do mês
  return pd.read_csv(url, sep = ';')

informes_diarios = busca_informes_cvm(2022,5)
informes_diarios

# Busca de cadastro dos fundos
def busca_cadastro_cvm(ano, mes, dia):
  url = 'http://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_{}{:02d}{:02d}.csv'.format(ano, mes, dia) # colocando o 0 na frente do mês
  return pd.read_csv(url, sep = ';', encoding = 'ISO-8959-1')

cadastro_cvm = busca_cadastro_cvm(2022, 5, 1)

"""**Definindo alguns filtros**"""

# Número de cotistas
# pivot()  refaz o dataframe original com os paramentros definidos por nós
# No arquivo original ela lista cada fundo pelo CNPJ e data  e cada fundo tá embaixo do outro, mal organizado
# Nós queremos um dataframe que tenha como índice os dias e a gente passando os outros valores
# Na coluna teremos os fundos e nas linhas os dias
# Os valores serão uma lista com cada elemento com a coluna que queremos os valores
fundos = informes_diarios[informes_diarios['NR_COTST'] >= minimo_cotistas].pivot(
    index = 'DT_COMPTC',
    columns = 'CNPJ_FUNDO',
    values = [
              'VL_TOTAL',
              'VL_QUOTA',
              'VL_PATRIM_LIQ',
              'CAPTC_DIA',
              'RESG_DIA'
    ])

fundos

# Valores das cotas
cotas_normalizadas = fundos['VL_QUOTA'] / fundos['VL_QUOTA'].iloc[0]
cotas_normalizadas

"""**Melhores e piores fundos de Abril de 2022**"""

# Melhores
melhores = pd.DataFrame()
melhores['retorno(%)'] = cotas_normalizadas.iloc[-1].sort_Values(ascending = False)[:5] - 1) * 100 # 5 primeiros fundos

# Listando os Fundos por CNPJ
for cnpj in melhores.index:
  fundo = cadastro_cvm[cadastro_cvm['CNPJ_FUNDO'] == cnpj]
  # usamops values[] para trazer somente o valor e não virem os índices e queremos somente o primeiro valor[0]
  melhores.at[cnpj, 'Fundo de Investimentos'] = fundo['DENOM_SOCIAL'].values[0]
  melhores.at[cnpj, 'Classe'] = fundo['CLASSE'].values[0]
  melhores.at[cnpj, 'PL'] = fundo['VL_PATRIM_LIQ'].values[0]
  melhores

# Piores
piores = pd.DataFrame()
piores['retorno(%)'] = cotas_normalizadas.iloc[-1].sort_Values(ascending = True)[:5] - 1) * 100 # 5 primeiros fundos

# Listando os Fundos por CNPJ
for cnpj in piores.index:
  fundo = cadastro_cvm[cadastro_cvm['CNPJ_FUNDO'] == cnpj]
  # usamops values[] para trazer somente o valor e não virem os índices e queremos somente o primeiro valor[0]
  piores.at[cnpj, 'Fundo de Investimentos'] = fundo['DENOM_SOCIAL'].values[0]
  piores.at[cnpj, 'Classe'] = fundo['CLASSE'].values[0]
  piores.at[cnpj, 'PL'] = fundo['VL_PATRIM_LIQ'].values[0]
  piores

"""# Webscraping Site da B3"""

import pandas as pd
pd.set_option('display.min_rows', 50)
pd.set_option('display.max_rows', 200)

# Beautiful Soup é projetado para fazer web scraping rapidamente
from bs4 import BeautifulSoup
import requests

# Verificador de SSL
# pip install --upgrade certifi

# import ssl
# ssl._create_default_https_context = ssl._create_unverified_context

url = 'https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br'
#pd.read_html(url, decimal=',', thousands='.')[0]

"""# Teste Tesouro Direto

**Fonte de dados:** https://www.tesourotransparente.gov.br/ckan/dataset
"""

# Bibliotecas
import pandas as pd
pd.set_option('display.max_colwidth', 150)
pd.set_option('display.min_rows', 20)

import matplotlib
matplotlib.style.use('seaborn-darkgrid')
matplotlib.rcParams['figure.figsize'] = (18, 8)

import plotly.graph_objects as go
import plotly.express as px

# Resolvendo problema do SSL
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

# Instalando Yahoo Finances
!pip install yfinance

# Biblioteca Yahoo Finance
import yfinance as yf

"""**Funções para busca e tratamento dos dados**"""

# Buscando títulos das taxas do Tesouro Direto
def busca_titulos():
  url = 'https://www.tesourotransparente.gov.br/ckan/dataset/df56aa42-484a-4a59-8184-7676580c81e3/resource/796d2059-14e9-44e3-80c9-2d9e30b405c1/download/PrecoTaxaTesouroDireto.csv'
  df = pd.read_csv(url, sep = ';', decimal = ',')
  df['Data Vencimento'] = pd.to_datetime(df['Data Vencimento'], dayfirst = True) # convertemos em datetime
  df['Data Base']  = pd.to_datetime(df['Data Base'], dayfirst = True) # convertemos em datetime
  multi_indice = pd.MultiIndex.from_frame(df.iloc[:, :3]) # separamos em Tipo, Data de vencimento e Data Base 
  df = df.set_index(multi_indice).iloc[:, 3:]
  return df

# Buscando Vendas do Tesouro Direto
def busca_vendas_tesouro():
  url = 'https://www.tesourotransparente.gov.br/ckan/dataset/f0468ecc-ae97-4287-89c2-6d8139fb4343/resource/e5f90e3a-8f8d-4895-9c56-4bb2f7877920/download/VendasTesouroDireto.csv'
  df = pd.read_csv(url, sep = ';', decimal = ',')
  df['Vencimento do Titulo'] = pd.to_datetime(df['Vencimento do Titulo'], dayfirst = True) # convertemos em datetime
  df['Data Venda']  = pd.to_datetime(df['Data Venda'], dayfirst = True) # convertemos em datetime
  multi_indice = pd.MultiIndex.from_frame(df.iloc[:, :3]) # separamos em Tipo, Data de vencimento e Data Base 
  df = df.set_index(multi_indice).iloc[:, 3:]
  return df

# Buscando Resgates do Tesouro Direto
def busca_recompras_tesouro():
  url = 'https://www.tesourotransparente.gov.br/ckan/dataset/f30db6e4-6123-416c-b094-be8dfc823601/resource/30c2b3f5-6edd-499a-8514-062bfda0f61a/download/RecomprasTesouroDireto.csv'
  df = pd.read_csv(url, sep = ';', decimal = ',')
  df['Vencimento do Titulo'] = pd.to_datetime(df['Vencimento do Titulo'], dayfirst = True) # convertemos em datetime
  df['Data Resgate']  = pd.to_datetime(df['Data Resgate'], dayfirst = True) # convertemos em datetime
  multi_indice = pd.MultiIndex.from_frame(df.iloc[:, :3]) # separamos em Tipo, Data de vencimento e Data Base 
  df = df.set_index(multi_indice).iloc[:, 3:]
  return df

"""**Preços e Taxas Históricas do Títulos Negociados**"""

# Busca dos dados
titulos = busca_titulos()

# Ordenando para facilitar busca
titulos.sort_index(inplace=True)
titulos

# Tipos de títulos
 tipos_titulos = titulos.index.droplevel(level=1).droplevel(level=1).drop_duplicates().to_list()
 tipos_titulos

"""**Selic**"""

selic2025 = titulos.loc[('Tesouro Selic', '2025-03-01')]
selic2025

selic2025['PU Base Manha'].plot()

"""**Pré-fixados 2023**"""

pre2023 = titulos.loc[('Tesouro Prefixado', '2023-01-01')]
pre2023

pre2023['PU Compra Manha'].plot()

pre2023['Taxa Compra Manha'].plot()

# Jogando ambos no mesmo gráfico e normalizando
(pre2023['Taxa Compra Manha'] / pre2023['Taxa Compra Manha'].iloc[0]).plot()
(pre2023['PU Compra Manha'] / pre2023['PU Compra Manha'].iloc[0]).plot()

"""**Pré-Fixados 2026**"""

pre2026 = titulos.loc[('Tesouro Prefixado', '2026-01-01')]
pre2026

pre2026['PU Compra Manha'].plot()

pre2026['Taxa Compra Manha'].plot()

# Jogando ambos no mesmo gráfico e normalizando
(pre2026['Taxa Compra Manha'] / pre2026['Taxa Compra Manha'].iloc[0]).plot()
(pre2026['PU Compra Manha'] / pre2026['PU Compra Manha'].iloc[0]).plot()

"""**IPCA+ 2045**"""

ipca2045 = titulos.loc[('Tesouro IPCA+', '2045-05-15')]
ipca2045

ipca2045['PU Compra Manha'].plot()

ipca2045['Taxa Compra Manha'].plot()

# Jogando ambos no mesmo gráfico e normalizando
(ipca2045['Taxa Compra Manha'] / ipca2045['Taxa Compra Manha'].iloc[0]).plot()
(ipca2045['PU Compra Manha'] / ipca2045['PU Compra Manha'].iloc[0]).plot()

"""**IPCA+ 2055 Com Juros Semestrais**"""

ipca2055 = titulos.loc[('Tesouro IPCA+ com Juros Semestrais', '2055-05-15')]
ipca2055

ipca2055['PU Compra Manha'].plot()

ipca2055['Taxa Compra Manha'].plot()

# Jogando ambos no mesmo gráfico e normalizando
(ipca2055['Taxa Compra Manha'] / ipca2055['Taxa Compra Manha'].iloc[0]).plot()
(ipca2055['PU Compra Manha'] / ipca2055['PU Compra Manha'].iloc[0]).plot()

"""**Vendas Tesouro Direto**"""

# Busca dos dados
vendas = busca_vendas_tesouro()

# Ordenando para facilitar busca
vendas.sort_index(inplace=True)
vendas

# Gráficos
vendas['Valor'].groupby('Data Venda').sum().plot()

"""**Recompras Tesouro Direto**"""

# Busca dos dados
recompras = busca_recompras_tesouro()

# Ordenando para facilitar busca
recompras.sort_index(inplace=True)
recompras

# Gráficos
recompras['Valor'].groupby('Data Resgate').sum().plot()

"""**Recompras Diárias Tesouro Selic 2025**"""

recompras_selic = recompras.loc[('Tesouro Selic', '2025-03-01')]

fig = px.bar(data_frame=recompras_selic, x=recompras_selic.index, y = 'Valor', labels = {'x': 'Data'})
fig.show()

# Agrupando por semana
recompras_selic_semanal = recompras_selic.resample('W').sum()

fig = px.bar(data_frame=recompras_selic_semanal, x=recompras_selic_semanal.index, y = 'Valor')
fig.show()

"""*Notamos que em Março de 2020 tiveram muitos resgates do Tesouro Selic, justamente no mês em que a bolsa mais caiu.*

*Vamos testar essa hipótese.*

**Resgates Tesouro Selic vs Índice Bovespa**
"""

# Pregamos o preço de fechamento ajustado, agrupamos semanalmente, com last() pegando o último dia(fechamento da sexta), calculando a variação % e por último, tiramos a última linha
ibov = yf.download('^BVSP')['Adj Close'].resample('W').last().pct_change()[1:]

# Criamos um novo dataframe, composto pelo dataframe da selic semanal; tiramos um percentual dos resgates dos títulos e agrupamos com a variação semanal do ibovespa no passo anterior
selic_ibov = pd.concat([recompras_selic_semanal['Valor'].pct_change()[1:], ibov], axis = 1).dropna()[1:]
selic_ibov.columns = ['Recompra(%)', 'IBOV(%)']
selic_ibov

# Plotando
fig = go.Figure(data = [
                        go.Bar(name = 'Recompras Selic(%)', x=selic_ibov.index, y=selic_ibov['Recompra(%)']),
                        go.Bar(name = 'IBOV(%)', x=selic_ibov.index, y=selic_ibov['IBOV(%)'])
])
fig.update_layout(barmode='group')
fig.show()

# Correlação
selic_ibov.corr()

"""# Impacto das maiores altas e quedas no Índice Bovespa e S&P500

*Excluindo os 10 piores e os 10 melhores dias*
"""

# Bibliotecas necessárias
import yfinance as yf
import plotly.graph_objects as go
import pandas as pd

# Função que pega uma série histórica de dados e calcula o retorno diário, criando uma coluna com retorno acumulado x cumprod(produto acumulado)
def calcula_retorno(dataframe):
  df = dataframe.copy() # série histórica de dados 
  df['daily_return'] = df.iloc[:,0].pct_change() # calculando retorno diário e coloca em porcentagem
  df['acum_return'] = (1 + df['daily_return']).cumprod() # criamos uma nova coluna com retorno acumulado, cumprod(produto acumulado)
  df['acum_return'].iloc[0] = 1 # colocamos o retorno acumulado igual a 1, para normalização
  df.rename(columns={df.columns[0]: 'price'}, inplace = True) # trocamos o nome da coluna para price
  return df

"""**S&P500**"""

sp500 = yf.download('^GSPC')[['Adj Close']]
sp500

sp500 = calcula_retorno(sp500)
sp500

# Plotando
fig = go.Figure() # startando uma figura
fig.add_trace(go.Scatter(x = sp500.index, y = sp500['price'])) #adicionando uma curva
fig.update_layout(yaxis_type='log') # plotando em escala logaritmica para melhor visualizar pois é um gráfico muito antigo com série histórica grande
fig.show()

"""**Melhores e piores dias da S&P500**

10 Melhores Dias
"""

sp500_melhores = sp500['daily_return'].sort_values(ascending=False)[:10]
sp500_melhores

"""10 Piores Dias"""

sp500_piores = sp500['daily_return'].sort_values(ascending=True)[:10]
sp500_piores

"""S&P500 **vs** S&P500 sem os 10 melhores dias **vs** S&P500 sem os 10 piores dias"""

# Removendo os 10 pores e 10 melhores
sp500_sem_melhores = (1 + sp500['daily_return'].drop(sp500_melhores.index)).cumprod()
sp500_sem_piores = (1 + sp500['daily_return'].drop(sp500_piores.index)).cumprod()

print('Retorno do S&P desde 1927: {:.2%}'.format(sp500['acum_return'].iloc[-1] - 1))
print('Retorno do S&P sem os 10 piores dias: {:.2%}'.format(sp500_sem_piores.iloc[-1] - 1))
print('Retorno do S&P sem os 10 melhores dias: {:.2%}'.format(sp500_sem_melhores.iloc[-1] - 1))

# Plot
fig = go.Figure()

fig.add_trace(go.Scatter(x = sp500.index, y=sp500['acum_return'], name = 'S&P500'))
fig.add_trace(go.Scatter(x = sp500_sem_melhores.index, y=sp500_sem_melhores.values, name = 'S&P500 sem 10 melhores dias'))
fig.add_trace(go.Scatter(x = sp500_sem_piores.index, y=sp500_sem_piores.values, name = 'S&P500 sem 10 piores dias'))
fig.show()

# Escala logaritmica
fig.update_layout(yaxis_type='log')
fig.show()

"""**Índice Bovespa**"""

# Série Histórica Índice Bovespa
ibov = yf.download('^BVSP', start = '1995-01-01')[['Adj Close']]

ibov = calcula_retorno(ibov)
ibov

fig = go.Figure()
fig.add_trace(go.Scatter(x = ibov.index, y=ibov['price']))
fig.update_layout(yaxis_type='log')
fig.show()

"""**Melhores e piores dias Índice Bovespa**

10 Melhores
"""

ibov_melhores =  ibov['daily_return'].sort_values(ascending=False)[:10]
ibov_melhores

"""**10 piores**"""

ibov_piores =  ibov['daily_return'].sort_values(ascending=True)[:10]
ibov_piores

"""IBOV **vs** IBOV sem os 10 melhores dias **vs** IBOV sem os 10 piores dias"""

# Removendo os 10 pores e 10 melhores
ibov_sem_melhores = (1 + ibov['daily_return'].drop(ibov_melhores.index)).cumprod()
ibov_sem_piores = (1 + ibov['daily_return'].drop(ibov_piores.index)).cumprod()

print('Retorno do IBOV desde 1995: {:.2%}'.format(ibov['acum_return'].iloc[-1] - 1))
print('Retorno do IBOV sem os 10 piores dias: {:.2%}'.format(ibov_sem_piores.iloc[-1] - 1))
print('Retorno do IBOV sem os 10 melhores dias: {:.2%}'.format(ibov_sem_melhores.iloc[-1] - 1))

# Plot
fig = go.Figure()

fig.add_trace(go.Scatter(x = ibov.index, y=ibov['acum_return'], name = 'IBOVESPA'))
fig.add_trace(go.Scatter(x = ibov_sem_melhores.index, y=ibov_sem_melhores.values, name = 'IBOV sem 10 melhores dias'))
fig.add_trace(go.Scatter(x = ibov_sem_piores.index, y=ibov_sem_piores.values, name = 'IBOV sem 10 piores dias'))
fig.show()

# Escala logaritmica
fig.update_layout(yaxis_type='log')
fig.show()

"""# FED - Federal Reserve Bank of ST. Louis"""

!pip install pandas_datareader

# Bibliotecas
import pandas_datareader.data as web
from scipy.stats import zscore  # para análise de desvios padrões
import plotly.express as px
import datetime

# Data início e Final
start = datetime.datetime(2000, 1, 1)
end = datetime.datetime(2013, 1, 27)

"""**PIB**"""

gdp = web.DataReader('GDP', 'fred', start)
gdp

px.line(gdp, x=gdp.index, y='GDP')

"""**Inflação**"""

cpi = web.DataReader('CPIAUCSL', 'fred', start)
cpi

px.bar(data_frame = cpi.pct_change()*100, x=cpi.index, y='CPIAUCSL', labels = {'CPIAUCSL':'Inflação(%)'})

"""**Treasury Bonds**"""

# Títulos de 3 meses
t3mo = web.DataReader('DGS3MO', 'fred', start)
t3mo

px.line(data_frame = t3mo, x=t3mo.index, y='DGS3MO')

# Títulos de 10 anos
t10 = web.DataReader('DGS10', 'fred', start)
t10

px.line(data_frame = t10, x=t10.index, y='DGS10')

"""**Emprego**"""

desemprego = web.DataReader('UNRATE', 'fred', start)
desemprego

var_desemprego = desemprego.pct_change()[1:]

# Gráfico de linhas
px.line(data_frame=desemprego, x = desemprego.index, y='UNRATE')

# Gráfico de barras
px.bar(data_frame=desemprego, x = desemprego.index, y='UNRATE')

px.bar(y=zscore(var_desemprego), x=var_desemprego.index, labels={'y':'desvios-padrão'})

"""**Pessoas Empregadas**"""

empregos = web.DataReader('PAYEMS', 'fred', start)
empregos

px.line(data_frame=empregos, x=empregos.index, y = 'PAYEMS')

px.bar(data_frame=empregos.pct_change(), x=empregos.index, y='PAYEMS')

"""**Impressão de Dólares - Agregado M2 (Papel moeda, depósitos à vista e à prazo)**"""

m2 = web.DataReader('M2', 'fred', start)
m2

px.line(data_frame=m2, x=m2.index, y='M2')

px.bar(data_frame=m2, x=m2.index, y='M2')

"""**Ativos do FED**"""

ativos = web.DataReader('WALCL', 'fred', start)
ativos

px.line(data_frame=ativos, x=ativos.index, y='WALCL')

px.bar(data_frame=ativos, x=ativos.index, y='WALCL')

"""---

# **2 - USANDO BIBLIOTECAS DO YAHOO FINANCE**
"""

# Instalando a biblioteca do Yahoo Finance
!pip install yfinance --upgrade --no-cache-dir

# Bibliotecas
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pandas_datareader.data as web # costuma ter um bug
# Consertando o erro do pandas_datareader
import yfinance as yf
yf.pdr_override() # sobrescreve as funções das buscas de cotação do YFinance pelo Pandas Datareader

# Obtendo dados
ibov = web.get_data_yahoo('^BVSP')
# ibov = web.get_data_yahoo('^BVSP', start = '2010-05-03', end = '2020-05-03') com intervalo definido

ibov.head()

ibov.tail()

# Plotando Preço de Fehamento
ibov['Close'].plot(figsize=(22,8))

"""----

# **3 - USANDO ALGUMAS FUNÇÕES**

**Médias Móveis, Filtros e Intervalos**
"""

# Médias móveis
# roooling() é uma janela móvel e o parãmetro são os dias
ibov['Close'].rolling(21).mean().plot(figsize=(22,8), label='NM21')
ibov['Close'].rolling(200).mean().plot(figsize=(22,8), label='NM200')
plt.legend()

# Fatiando o código acima com um filtro
ibov_fatiado = ibov[ibov.index.year == 2008]
ibov_fatiado['Close'].rolling(21).mean().plot(figsize=(22,8), label='NM21')
ibov_fatiado['Close'].rolling(200).mean().plot(figsize=(22,8), label='NM200')
plt.legend()

# Selecionando um intervalo com o código acima
ibov_intervalo = ibov[(ibov.index.year >= 2008) & (ibov.index.year <= 2015)]
ibov_intervalo['Close'].rolling(21).mean().plot(figsize=(22,8), label='NM21')
ibov_intervalo['Close'].rolling(200).mean().plot(figsize=(22,8), label='NM200')
plt.legend()

# Todos os dias dos meses de dezembro da série temporal
ibov_dezembro = ibov[(ibov.index.month == 12) & (ibov.index.year >= 2008) & (ibov.index.day == 14)]
ibov_dezembro.plot(figsize=(22,15), xlabel='Todos os Dias 14 de Dezembro Partindo de 2008')
plt.legend()

# Obtendo dados da VALE (sempre usar o .SA)
vale = web.get_data_yahoo('VALE3.SA')

vale.head()

"""**Correlação**

*Calculando a correlação entre Dólar e Ibovespa (Ibovespa Dolarizado)*
"""

tickers = ['^BVSP', 'USDBRL=X']
# Como a série do USDBRL=X não tem dados concretos até 2007, criaremos uma data inicial
carteira = web.get_data_yahoo(tickers, start='2007-06-01')['Close']

carteira

# Exclindo os valor missing
carteira = carteira.dropna()
carteira

carteira.plot(subplots=True, figsize=(22,8))

# Mudando os nomes das colunas
carteira.columns = ['DOLAR', 'IBOV']
carteira

carteira.plot(subplots=True, figsize=(22,8))

# Importando Seaborn para melhorar os gráficos
import seaborn as sns

sns.set()
carteira.plot(subplots=True, figsize=(22,8))

# Correlação
sns.heatmap(carteira.corr(), annot=True) # annot() coloca os valores dentro dos quadrados

# Como pegamos a base toda para a correlação (que varia durante o tempo)
# Então criamos janelas de observação
# Com média móvel rolling() contendo os 252 dias úteis do ano
carteira['DOLAR'].rolling(252).corr(carteira['IBOV']).plot(figsize = (22, 8))

# Criando uma coluna para IBOVESPA DOLARIZADO
carteira['IBOV_DOLARIZADO'] = (carteira['IBOV'] / carteira['DOLAR'])
carteira.plot(figsize=(22,8))

# Fatiado
sns.set()
carteira.plot(subplots=True, figsize=(22,8))

"""# **4 - USANDO ESTATÍSTICA**

# **5 - IBGE**

Fontes: 

https://servicodados.ibge.gov.br/api/docs

https://servicodados.ibge.gov.br/api/docs/agregados?versao=3


https://servicodados.ibge.gov.br/api/docs/projecoes

Query Builder

https://servicodados.ibge.gov.br/api/docs/agregados?versao=3#api-bq

Matplotlib

https://matplotlib.org/

**Gráfico de Área e Densidade Demográfica**
"""

# Bibliotecas
import requests
import pandas as pd #Data handling 
import matplotlib.pyplot as plt
import numpy as np #Array handling
import json
import seaborn as sns

url = 'https://servicodados.ibge.gov.br/api/v3/agregados/1301/periodos/2010/variaveis/615|616?localidades=N2[all]'
url

Data_IBGE = requests.get(url).json()
Data_IBGE
# trata-se de uma matriz com as 2 elementos(selecionado no site do IBGE para essa pesquisa)

# Obtendo o título através dos níveis do código
titulo = Data_IBGE[0]['variavel']
titulo

unidVar0 = Data_IBGE[0]['unidade']
unidVar1 = Data_IBGE[1]['unidade']

unidVar0

unidVar1

RR = 4 # Posição 4
Area_total = Data_IBGE[0]['resultados'][0]['series'][RR]['serie']['2010']
Area_total

PR = 1 # posição 1
area_total1 = Data_IBGE[0]['resultados'][0]['series'][PR]['serie']['2010']
area_total1

# Obtendo a região
Regiao = Data_IBGE[0]['resultados'][0]['series'][RR]['localidade']['nome']
Regiao

# Printando as informações
print('A região', Regiao, 'tem',  Area_total, unidVar0)
print('A região', Regiao, 'tem',  Area_total, unidVar1)

"""**Preparando os dados para representar em gráfico**"""

# faremos uma união das informações para poder produzir o gráfico de melhor maneira
dados = []

# Testando os níveis 
len(Data_IBGE[0]['resultados'][0]['series'])

# Testando o laço
for i in range(len(Data_IBGE[0]['resultados'][0]['series'])):
  print(i)

# Enchendo dados[] com informação com região, área total e km²
for i in range(len(Data_IBGE[0]['resultados'][0]['series'])): # criando uma matriz com o vetor dos dados
  dados.append(Data_IBGE[0]['resultados'][0]['series'][i]['localidade']['nome']) # qual a região
  dados.append(Data_IBGE[0]['resultados'][0]['series'][i]['serie']['2010']) # área total das unidades territorias
  dados.append(Data_IBGE[1]['resultados'][0]['series'][i]['serie']['2010']) # habitante por quilômetro quadrado

dados

# Usando uma matriz para visualizarmos melhor os dados
# Numpy é a biblioteca para trabalhar com matrizes
matriz_np = np.array(dados)
matriz_np

# Ajustando a matriz
matriz_ajustada = np.reshape(matriz_np, (5, 3))
matriz_ajustada

# Adicionando nomes para colunas e armazenando tudo em dataframe
varName0 = Data_IBGE[0]['resultados'][0]['series'][i]['localidade']['nivel']['nome']
varName1 = Data_IBGE[0]['variavel']
varName2 = Data_IBGE[1]['variavel']

colunas = [varName0, varName1, varName2]
colunas

df = pd.DataFrame(matriz_ajustada, columns = colunas)
df

"""**Gráfico de barras simples**"""

labels = np.array(df[varName0]) # transformamos os dados do dataframe de volta para um array
Var1 = np.array(df[varName1],dtype=float) # valores propriamente ditos e redefinimos os dados para float

x = np.arange(len(labels))  # quanto dados tem dentro de labels
width = 0.5  # largura das barras

fig, ax = plt.subplots() # aqui salvamos a figura do gráfico em fig e ax é o gráfico em si
ax.bar(x = x , height = Var1, width = width)

ax.set_ylabel(unidVar0 )
ax.set_title(varName1)
ax.set_xticks(x)
ax.set_xticklabels(labels)
fig.tight_layout()
plt.show()

"""**Gráfico de barras agrupadas**"""

labels = np.array(df[varName0]) # transformamos os dados do dataframe de volta para um array
Var1 = np.array(df[varName1], dtype=float) # Área total das unidades territoriais
Var2 = np.array(df[varName2], dtype=float) # Densidade demográfica da unidade territorial
Cl1 = 'b'
Cl2 = 'tab:red' #color

x = np.arange(len(labels))  # quantas barras aparecerão
width = 0.35  # largura das barras

# Barras do lado esquerdo (Km²)
fig, ax = plt.subplots()
ax.bar(x - width/2, Var1, width, label= labels, color = Cl1)
ax.set_ylabel(unidVar0, color = Cl1)
ax.set_title('Área x densidade')
ax.set_xticks(x)
ax.set_xticklabels(labels)

# Barras do lado esquerdo (Habitantes por Km²)
ax2 = ax.twinx() # twinx() espelha o gráfico para o lado direito
ax2.bar(x + width/2, Var2, width, label= labels, color=Cl2 )
ax2.set_ylabel(unidVar1, color = Cl2) 

fig.tight_layout()
plt.show()

"""**Gráfico setorial (Pizza)**"""

labels = np.array(df[varName0]) # rótulos
Var1 = np.array(df[varName1], dtype=float) # variáveis
explode = (0.1, 0, 0, 0, 0)  # explode() separa as fatias 
fig1, ax1 = plt.subplots() 
ax1.pie(Var1, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.set_title(varName1)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

# Dois gráficos setoriais
labels = np.array(df[varName0])
Var1 = np.array(df[varName1], dtype=float)
Var2 = np.array(df[varName2], dtype=float)

explode1 = (0.1, 0, 0, 0, 0)  # only "explode" the 1st slice (i.e. 'Norte')
explode2 = (0, 0, 0.1, 0, 0)

CC= ['C0', 'C1', 'C2', 'C3', 'C4']#Colors
fig1, ax1 = plt.subplots(1, 2, figsize=(7, 7)) # definimos quantos gráficos tem dentro da figura

## --- Gráfico 1
ax1[0].pie(Var1, explode=explode1, labels=labels, colors= CC, autopct='%1.1f%%',
         shadow=True, startangle=90) # explode() separa as fatias
ax1[0].set_title(varName1)
ax1[0].set_position([0, 0.5, 0.5, 0.6])#[left, bottom, width, height

 ## --- Gráfico 2
ax1[1].pie(Var2, explode=explode2, labels=labels, colors= CC, autopct='%1.1f%%',
         shadow=True, startangle=90) # explode() separa as fatias
ax1[1].set_title(varName2)
ax1[1].set_position([0.7, 0.5, 0.5, 0.6])
plt.show()

"""**Gráficos: Estimativas de população IBGE**"""

urlPop = 'https://servicodados.ibge.gov.br/api/v3/agregados/6579/periodos/2001|2002|2003|2004|2005|2006|2008|2009|2011|2012|2013|2014|2015|2016|2017|2018|2019|2020|2021/variaveis/9324?localidades=N3[41]'
Data_IBGE_P = requests.get(urlPop).json() # requisição

# Data_IBGE_P
titulo = Data_IBGE_P[0]['variavel']

# titulo
unid = Data_IBGE_P[0]['unidade']
unid

Ano = 2011 #Qual região
Pop_ano = Data_IBGE_P[0]['resultados'][0]['series'][0]['serie'][str(Ano)]
Pop_ano
Localidade = Data_IBGE_P[0]['resultados'][0]['series'][0]['localidade']['nome']
print('A', titulo, 'no', Localidade, 'em', Ano, 'era de',  Pop_ano, unid)

"""**Gráfico de Linhas**"""

DT = Data_IBGE_P[0]['resultados'][0]['series'][0]['serie']
names = list(DT.keys())
values = list(DT.values())

#parse string to float
Vyear = np.zeros(len(names))
Vvalue = np.zeros(len(names))
for i in range(len(names)):
   Vyear[i] = int(names[i])
   Vvalue[i] = values[i]

# -- 	População recenseada no último censo [2010]
Pop_2010 = 10444526 #https://cidades.ibge.gov.br/brasil/pr/panorama


LT1 = np.arange(2001, 2022, step=1);#Axis range and interval

fig, axs = plt.subplots(figsize=(10, 5))
# axs.bar(Vyear, Vvalue)
axs.scatter(2010, Pop_2010, color = 'r')
axs.plot(Vyear, Vvalue, marker = 'o')
axs.set_ylabel(unid)
axs.set_xlabel('Ano')
axs.set_xlim(2000, 2022)
axs.set_ylim(Vvalue[0]-1e5, Vvalue[18]+1e5)
axs.set_xticks(LT1)
axs.legend(['Estimada', 'Censo 2010'])

fig.suptitle(titulo+' e recenseada no '+ Localidade)

"""**Gráfico: Pirâmide etária**"""

url = 'https://servicodados.ibge.gov.br/api/v3/agregados/2149/periodos/2000/variaveis/289?localidades=N3[41]&classificacao=303[6854]|2[4,5]|58[all]'
Data_IBGE = requests.get(url).json()
Data_IBGE

# Data_IBGE
titulo = Data_IBGE[0]['variavel']
titulo
unidVar0 = Data_IBGE[0]['unidade']
# unidVar1 = Data_IBGE[1]['unidade']
unidVar0
RR = 2
Var1 = Data_IBGE[0]['resultados'][RR]['classificacoes'][1]['categoria']['4']#Sexo
Var2 = Data_IBGE[0]['resultados'][RR]['classificacoes'][2]['categoria']['1141']#Faixa etária
Var3 = Data_IBGE[0]['resultados'][RR]['series'][0]['serie']['2000']#Valor
Var1, Var2, Var3 
# Regiao = Data_IBGE[0]['resultados'][0]['series'][RR]['localidade']['nome']

# Preparar dados para representar em um gráfico
dados = []
n1 = np.array((range(1140, 1156, 1)))
n2 = [2503]
vv = [*n1, *n2]

PP = 34
for i in range( PP ):#cria uma matriz com vetor de dados
  if i<17: 
    S = '4'
    L = i
    C = 1
  else:
    S = '5'
    L = i-17
    i = i + 1 
    C = -1  
  dados.append( Data_IBGE[0]['resultados'][i+1]['classificacoes'][1]['categoria'][S] )#Sexo
  dados.append( Data_IBGE[0]['resultados'][i+1]['classificacoes'][2]['categoria'][str(vv[L])] )#Faixa etária
  dados.append( pd.to_numeric(Data_IBGE[0]['resultados'][i+1]['series'][0]['serie']['2000'])*C )#Valor

matriz_np = np.array(dados)
# matriz_np
matriz_ajustada = np.reshape(matriz_np, (PP, 3))
# matriz_ajustada
## -- Acrescentar nomes para colunas e armazenar tudo em um DataFrame
varName0 = Data_IBGE[0]['resultados'][RR]['classificacoes'][1]['nome']#Sexo
varName1 = Data_IBGE[0]['resultados'][1]['classificacoes'][2]['nome']#Faixa etaria
varName2 = Data_IBGE[0]['variavel']
# colunas = [varName0, varName1, varName2]

colunas = [varName0, 'Faixa_Etaria', 'Brasileiros_natos']
df = pd.DataFrame(matriz_ajustada, columns = colunas)
# df

# Draw Plot
# Código original (https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/#29.-Population-Pyramid)
plt.figure(figsize=(10,8), dpi= 80)

group_col = varName0
order_of_bars = df.Faixa_Etaria.unique()[::-1]

# colunas = [varName0, 'Faixa_Etaria', 'Brasileiros_natos']
df[colunas[2]] = pd.to_numeric(df[colunas[2]])

colors = [plt.cm.Spectral(i/float(len(df[group_col].unique())-1)) for i in range(len(df[group_col].unique()))]

for c, group in zip(colors, df[group_col].unique()):
    sns.barplot(x=colunas[2], y= colunas[1], data=df.loc[df[group_col]==group, :], order=order_of_bars, color=c, label=group)

# Decorations    
plt.xlabel(colunas[2])
plt.ylabel(colunas[1])
plt.yticks(fontsize=12)
plt.title("Brasileiros natos no Paraná por sexo e grupos de idade", fontsize=22)
plt.legend()
plt.show()

"""# Teste IBGE API

Endereço da API:

https://servicodados.ibge.gov.br/api/v1/projecoes/populacao
"""

import urllib3
import json
import matplotlib.pyplot as plt
import numpy as np

# Preparando para receber o endereço do Endpointing
http = urllib3.PoolManager()
urlPortalApiIBGE = 'https://servicodados.ibge.gov.br/api/v1/projecoes/populacao'
response = http.request('GET', urlPotalApiIBGE)
print('Response Status: ', response.status)

# O que vem da consulta da biblioteca
data_response = response.data.decode('utf-8') # acertando a acentuação
data_response

# Transformando em JSON
data_json = json.loads(data_response)
data_json

# Pegando um horário
print(data_json['horario'])

print(data_json['projecao']['periodoMedio'])

grafico = data_json['projecao']['periodoMedio']
grafico

nascimento = grafico['nascimento']
obito = grafico['obito']
atualizacao = data_json['horario'] # tem que ser json por ser horário
print(obito)
print(nascimento)
print(atualizacao)

labels = [atualizacao]
men_means = [nascimento]
women_means = [obito]

x = np.arange(len(labels))  # the label locations
width = 0.35  # the width of the bars

fig, ax = plt.subplots(figsize = (15, 10))
rects1 = ax.bar(x - width/2, nascimento, width, label='Total de Nascimentos')
rects2 = ax.bar(x + width/2, obito, width, label='Total de Óbitos')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Quantidades')
ax.set_title('IBGE Nascimentos x Óbitos')
ax.set_xticks(x, labels)
ax.legend()

ax.bar_label(rects1, padding=3)
ax.bar_label(rects2, padding=3)

fig.tight_layout()

plt.show()

"""**API IBGE - Pesquisas (Educação)**

https://servicodados.ibge.gov.br/api/docs/pesquisas
"""

import urllib3
import json
import matplotlib.pyplot as plt
import numpy as np

# Preparando para receber o endereço do Endpointing
http = urllib3.PoolManager()
# Salvando o endereço
api_educacao = 'https://servicodados.ibge.gov.br/api/v1/pesquisas/{pesquisa}/indicadores/'
response = http.request('GET', api_educacao)
print('Response Status: ', response.status)

api_educacao

# O que vem da consulta da biblioteca
data_response = response.data.decode('utf-8') # acertando a acentuação
data_response

# Transformando em JSON
data_json2 = json.loads(data_response)
data_json2

"""# Teste com o SIDRA (IBGE)

Fonte:

https://sidra.ibge.gov.br/home/pms/brasil

Selecionamos a tabela **4094** para realizar o trabalho
"""



"""# **6 - USANDO O TRADING ECONOMICS**"""

# Instalando a biblioteca
!pip3 install tradingeconomics

import tradingeconomics as te

"""# **7 - USANDO O QUANTSTATS**

**Repositório**:

https://github.com/ranaroussi/quantstats
"""

!pip install quantstats

import quantstats as qs

qs.extend_pandas()

"""Usando os ticks do próprio Yahoo Finance"""

itsa = qs.utils.download_returns('itsa4.sa')
itsa

# Método do próprio quantstats
itsa.monthly_returns() # períodos de retornos

# Plotando
itsa.plot_monthly_heatmap(figsize = (15, 5))

# Todas as funções disponíveis
dir(qs.stats)

itsa.volatility()

# Relatório BÁSICO
qs.reports.basic(itsa, benchmark='^BVSP') # retornos e benchmark

# Relatório COMPLETO
qs.reports.full(itsa, benchmark='^BVSP') # retornos e benchmark

itsa.plot_rolling_beta('^BVSP')

# Retorno não composto
qs.stats.cagr(itsa)